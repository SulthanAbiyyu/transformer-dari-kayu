{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "text = \"time flies like an arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30522\n",
    "HIDDEN_SIZE = 768\n",
    "MAX_POSITION = 512\n",
    "NUM_HEADS = 12\n",
    "INTERMEDIATE_SIZE = 3072\n",
    "DROPOUT_P = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2051, 10029,  2066,  2019,  8612]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_position):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(max_position, embed_dim)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_len = input_ids.size(-1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long).view(1, -1)\n",
    "\n",
    "        token_embed = self.token_embeddings(input_ids)\n",
    "        position_embed = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = token_embed + position_embed\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim, mask=None):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.q(x); k = self.k(x); v = self.v(x)\n",
    "        d_k = q.size(-1) # head_dim\n",
    "        scores = torch.bmm(q, k.transpose(1, 2)) / d_k ** 0.5\n",
    "\n",
    "        if self.mask is not None:\n",
    "            scores = scores.masked_fill(self.mask==0, -float(\"inf\"))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        return torch.bmm(weights, v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mask=None):\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(embed_dim, self.head_dim, mask) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_fc = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.out_fc(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size, dropout_p):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(embed_dim, hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, embed_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_in(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_out(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, intermediate_size, dropout_p):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, intermediate_size, dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.layernorm_1(x))\n",
    "        x = x + self.ff(self.layernorm_2(x))\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, intermediate_size, dropout_p):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_3 = nn.LayerNorm(embed_dim)\n",
    "        self.mask = torch.tril(torch.ones(5, 5))\n",
    "        self.mha_1 = MultiHeadAttention(embed_dim, num_heads, mask=self.mask)\n",
    "        self.mha_2 = MultiHeadAttention(embed_dim, num_heads, mask=self.mask)\n",
    "        self.ff = FeedForward(embed_dim, intermediate_size, dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha_1(self.layernorm_1(x))\n",
    "        x = x + self.mha_2(self.layernorm_2(x))\n",
    "        x = x + self.ff(self.layernorm_3(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = Embeddings(config.vocab_size, config.hidden_size, config.max_position_embeddings)\n",
    "inputs_embedded = embed(inputs.input_ids)\n",
    "inputs_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EncoderBlock(config.hidden_size,\n",
    "                   config.num_attention_heads, config.intermediate_size, config.hidden_dropout_prob)\n",
    "enc_result = enc(inputs_embedded)\n",
    "enc_result.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denc = DecoderBlock(config.hidden_size, config.num_attention_heads, config.intermediate_size, config.hidden_dropout_prob)\n",
    "out = denc(inputs_embedded)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(4,2)\n",
    "q = nn.Linear(2, 5)\n",
    "q(data).size(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_25016\\2933082444.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3120404ab40088dde46363bad1bdd78d1aeca7b6f18e79999fc72cb3d9151a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
